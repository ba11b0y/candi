{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUmiF_o0dH9B"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"http://introtodeeplearning.com\">\n",
        "        <img src=\"https://i.ibb.co/Jr88sn2/mit.png\" style=\"padding-bottom:5px;\" />\n",
        "      Visit MIT Deep Learning</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/MITDeepLearning/introtodeeplearning/blob/master/lab3/LLM_Finetuning.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/MITDeepLearning/introtodeeplearning/blob/master/lab3/LLM_Finetuning.ipynb\">\n",
        "        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
        "</table>\n",
        "\n",
        "# Copyright Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBNhP2AodH9C"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 MIT Introduction to Deep Learning. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the MIT License. You may not use this file except in compliance\n",
        "# with the License. Use and/or modification of this code outside of MIT Introduction\n",
        "# to Deep Learning must reference:\n",
        "#\n",
        "# © MIT Introduction to Deep Learning\n",
        "# http://introtodeeplearning.com\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmkjWI4fVeAh",
        "outputId": "dbb8d446-731f-4cb5-cf29-46dbb627c57a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "# Install and import MIT Deep Learning utilities\n",
        "!pip install mitdeeplearning > /dev/null 2>&1\n",
        "import mitdeeplearning as mdl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oo64stjwBvnB",
        "outputId": "21cb5c3d-24f4-402e-dbf9-0136ec78ad52"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from lion_pytorch import Lion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TN2zHVhfBvnE",
        "outputId": "16e48233-d37d-4cde-c8d8-98212638bc07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<start_of_turn>user\n",
            "What is your name?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "My name is Gemma!<end_of_turn>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Basic question-answer template\n",
        "template_without_answer = \"<start_of_turn>user\\n{question}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "template_with_answer = template_without_answer + \"{answer}<end_of_turn>\\n\"\n",
        "\n",
        "# Let's try to put something into the template to see how it looks\n",
        "print(template_with_answer.format(question=\"What is your name?\", answer=\"My name is Gemma!\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeDF1JI-BvnF",
        "outputId": "65b7375a-8a48-4aba-b16a-3e3879f5d7e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "<frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
            "<frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 256000\n"
          ]
        }
      ],
      "source": [
        "# Load the tokenizer for Gemma 2B\n",
        "model_id = \"unsloth/gemma-2-2b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Add a padding token if it doesn't exist\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# How big is the tokenizer?\n",
        "print(f\"Vocab size: {len(tokenizer.get_vocab())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JH1XzPkiBvnF",
        "outputId": "dcf082a5-dd67-4c9b-ef37-107e2097ccb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text: Here is some sample text!\n",
            "Encoded tokens: tensor([[     2,   4858,    603,   1009,   6453,   2793, 235341]])\n",
            "Decoded text: Here is some sample text!\n"
          ]
        }
      ],
      "source": [
        "# Lets test out both steps:\n",
        "text = \"Here is some sample text!\"\n",
        "print(f\"Original text: {text}\")\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "print(f\"Encoded tokens: {tokens}\")\n",
        "\n",
        "# Decode the tokens\n",
        "decoded_text = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
        "print(f\"Decoded text: {decoded_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LybwmysGdH9D"
      },
      "source": [
        "This is really cool. Now we have a way to move in and out of the token space.\n",
        "\n",
        "To \"chat\" with our LLM chatbot, we need to use the tokenizer and the chat template together, in order for the model to respond to the user's question. We can use the templates defined earlier to construct a prompt for the model, without the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyBxl6NIBvnF",
        "outputId": "616f4858-4f4f-4ac3-954b-a196feb11575"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<start_of_turn>user\n",
            "What is the capital of France? Use one word.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = template_without_answer.format(question=\"What is the capital of France? Use one word.\")\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcqeF4aqdH9E"
      },
      "source": [
        "If we were to feed this to the model, it would see that it is now the start of the model's turn, and it would generate the answer to this question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWtWvgiuBvnG",
        "outputId": "cdb79dbc-5167-4559-951b-621a4da0f8fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 10,383,360 || all params: 2,624,725,248 || trainable%: 0.3956\n"
          ]
        }
      ],
      "source": [
        "# Load the model -- note that this may take a few minutes\n",
        "# Load the model -- note that this may take a few minutes\n",
        "def apply_lora(model):\n",
        "    # Define LoRA config\n",
        "    lora_config = LoraConfig(\n",
        "        r=8, # rank of the LoRA matrices\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\n",
        "            \"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Apply LoRA to the model\n",
        "    lora_model = get_peft_model(model, lora_config)\n",
        "    return lora_model\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "# Optional: enable gradient checkpointing to save memory\n",
        "if hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "    model.gradient_checkpointing_enable()\n",
        "# Resize embeddings (pad token added) and attach LoRA adapters\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model = apply_lora(model)\n",
        "\n",
        "if hasattr(model, \"print_trainable_parameters\"):\n",
        "    model.print_trainable_parameters()\n",
        "else:\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Trainable params: {trainable} / {total} ({trainable/total*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SMDd5dpBvnG",
        "outputId": "4c27847f-9523-4609-f3e5-b8192367399d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user\n",
            "What does MIT stand for?\n",
            "model\n",
            "MIT stands for **Massachusetts Institute of Technology**. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "### Putting it together to prompt the model and generate a response ###\n",
        "\n",
        "# 1. Construct the prompt in chat template form\n",
        "question = \"What does MIT stand for?\"\n",
        "prompt = template_without_answer.format(question=question)\n",
        "\n",
        "# 2. Tokenize the prompt, including attention mask\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
        "\n",
        "# 3. Generate a sequence of tokens for the answer\n",
        "with torch.no_grad():\n",
        "    gen_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],  # Add attention mask\n",
        "        max_new_tokens=50,  # Increased tokens\n",
        "        do_sample=True,  # Enable sampling\n",
        "        temperature=0.7 # Added temperature\n",
        "    )\n",
        "\n",
        "# 4. Decode and print the full text\n",
        "print(tokenizer.decode(gen_ids[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnWMUQVbBvnG",
        "outputId": "18c82db0-5816-4de0-a731-d9cb0b246a64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bos><start_of_turn>user\n",
            "What does MIT stand for?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "MIT stands for **Massachusetts Institute of Technology**. \n",
            "<end_of_turn>\n"
          ]
        }
      ],
      "source": [
        "prompt = template_without_answer.format(question=\"What does MIT stand for?\")\n",
        "tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "output = model.generate(tokens, max_new_tokens=20)\n",
        "print(tokenizer.decode(output[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fb6Y679hBvnI"
      },
      "outputs": [],
      "source": [
        "# LoRA is a way to finetune LLMs very efficiently by only updating a small subset of the model's parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcg-jFy6dH9E"
      },
      "source": [
        "### 1.3.3: Forward pass and loss computation\n",
        "\n",
        "Now let's define a function to perform a forward pass through the LLM and compute the loss. The forward pass gives us the logits -- which reflect the probability distribution over the next token -- for the next token. We can compute the loss by comparing the predicted logits to the true next token -- our target label. Note that this is effectively a classification problem! So, our loss can be captured by the cross entropy loss, and we can use PyTorch's [`nn.functional.cross_entropy`](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html) function to compute it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCLtZwxwBvnI"
      },
      "outputs": [],
      "source": [
        "def forward_and_compute_loss(model, tokens, mask, context_length=512):\n",
        "    # Truncate to context length\n",
        "    tokens = tokens[:, :context_length]\n",
        "    mask = mask[:, :context_length]\n",
        "\n",
        "    # Construct the input, output, and mask\n",
        "    x = tokens[:, :-1]\n",
        "    y = tokens[:, 1:]\n",
        "    mask = mask[:, 1:]\n",
        "\n",
        "    # Forward pass to compute logits\n",
        "    logits = model(x).logits\n",
        "\n",
        "    # Compute loss\n",
        "    loss = F.cross_entropy(\n",
        "        logits.view(-1, logits.size(-1)),\n",
        "        y.view(-1),\n",
        "        reduction=\"none\"\n",
        "    )\n",
        "\n",
        "    # Mask out the loss for non-answer tokens\n",
        "    loss = loss[mask.view(-1)].mean()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeoCuqmxN_NH"
      },
      "source": [
        "## PII Masking Fine-tuning (source_text ➜ target_text)\n",
        "\n",
        "Adapt the pipeline to a PII masking dataset with columns: `source_text`, `target_text`, `privacy_mask`, `span_labels`, `mbert_text_tokens`, `mbert_bio_labels`, `id`, `language`, `set`. We will fine-tune the model to transform `source_text` into its masked form `target_text`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F28CS54wN_NH"
      },
      "outputs": [],
      "source": [
        "# Prompt template specialized for PII masking with few-shot examples\n",
        "\n",
        "def build_pii_prompt(source_text: str) -> str:\n",
        "    instruction = (\n",
        "        \"You are a data privacy assistant. Mask all personally identifiable information (PII) \"\n",
        "        \"in the following text using the masking scheme shown in the examples. \"\n",
        "        \"Output only the masked text.\\n\\n\"\n",
        "        \"Examples:\\n\\n\"\n",
        "        \"Input: My name is John Smith and my email is john.smith@email.com\\n\"\n",
        "        \"Output: My name is [FIRSTNAME] [LASTNAME] and my email is [EMAIL]\\n\\n\"\n",
        "        \"Input: Call me at 555-123-4567 or visit 123 Main Street, Boston MA 02101\\n\"\n",
        "        \"Output: Call me at [PHONENUMBER] or visit [STREET] [CITY] [STATE] [ZIPCODE]\\n\\n\"\n",
        "        \"Input: My username is alice_2023 and I was born on 03/15/1990\\n\"\n",
        "        \"Output: My username is [USERNAME] and I was born on [DOB]\\n\\n\"\n",
        "        \"Input: The SSN is 123-45-6789 and account number is ACC98765\\n\"\n",
        "        \"Output: The SSN is [SSN] and account number is [ACCOUNTNUMBER]\"\n",
        "    )\n",
        "    return (\n",
        "        f\"<start_of_turn>user\\n{instruction}\\n\\nText:\\n{source_text}\\n<end_of_turn>\\n\"\n",
        "        f\"<start_of_turn>model\\n\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258,
          "referenced_widgets": [
            "6053a66cd5f24d948898ef268123756d",
            "5659cdd59f254996b2dd9a98bbbbb25f",
            "75e3bfafa830433aace87dbf281d6e2b",
            "95afe5626c384b8bb8ccaf3b83042dda",
            "887eda8720584cfeb971a33a5a0bef7a",
            "c23173319385439d900101868a59de08",
            "de070153197343c5814cfebc63db2cf1",
            "4a55f19eead1401196dc5acbd8646261",
            "faf3e9594b404893b07e88cb52e80d73",
            "894d667575eb4fcb8e365bdfe1805279",
            "b903ec5a163341e9b23a9c11e82869d4",
            "86e46d5a1b904b2fac55544fcd7dfdaf",
            "f82d2ba01af041fb9cc2854032d7fa29",
            "b9a0b6bd950b4a05b8c734c78a2ccb5b",
            "e91d3762affb4b83ad77c8de4ed78eab",
            "23fe6f9230b24c18b6c91f7dd4bdaa2f",
            "9c13440fecd1409492fed73cefb3d65b",
            "9cba3a4eb9ff475cabf0944c043b5f58",
            "91862fc651c3438ea2b3d3de2cdb78a2",
            "958837a29c124951a72ed9141b93ae62",
            "4236993b39e941f7a59b73df72c59bcd",
            "423762a3ddaa448599d7120fe2294385",
            "a26e5be011614cca88dbd8b408a2721f",
            "181bb82ce4134150871ec9836bc01938",
            "f509e4aa420246998d6b6e110fbd8463",
            "e63ed00e17694fc192cb5eff8126a543",
            "62c188b0176b4bbf8914f69543dceda8",
            "168042414ded436c8669b0b1366beca3",
            "3047ddc9c80f4dcc8bcbdd6bee2fa083",
            "0bb8b7d0724d4691a91b3ca251a14b10",
            "9aa0d7c8360f470f8aa0baf79cc1191c",
            "da93275a012f4eccb3671d5f9235feb0",
            "b25aa3b07d35461bba5fff489cb02ce5"
          ]
        },
        "id": "WPepdopuN_NH",
        "outputId": "a96c9410-c573-4cf4-a9cf-70936f2247ef"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6053a66cd5f24d948898ef268123756d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Filter:   0%|          | 0/7946 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86e46d5a1b904b2fac55544fcd7dfdaf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/29908 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a26e5be011614cca88dbd8b408a2721f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/7946 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PII dataset: Dataset({\n",
            "    features: ['source_text', 'target_text', 'privacy_mask', 'span_labels', 'mbert_text_tokens', 'mbert_bio_labels', 'id', 'language', 'set', 'prompt', 'target'],\n",
            "    num_rows: 29908\n",
            "})\n",
            "Columns: ['source_text', 'target_text', 'privacy_mask', 'span_labels', 'mbert_text_tokens', 'mbert_bio_labels', 'id', 'language', 'set', 'prompt']\n",
            "{'prompt': '<start_of_turn>user\\nYou are a data privacy assistant. Mask all personally identifiable information (PII) in the following text using the masking scheme shown in the examples. Output only the masked text.\\n\\nText:\\nSubject: Group Messaging for Admissions Process\\n\\nGood morning, everyone,\\n\\nI hope this message finds you well. As we continue our admissions processes, I would like to update you on the latest developments and key information. Please find below the timeline for our upcoming meetings:\\n\\n- wynqvrh053 - Meeting at 10:20am\\n- luka.burg - Meeting at 21\\n- qahil.wittauer - Meeting at quarter past 13\\n- gholamhossein.ruschke - Meeting at 9:47 PM\\n- pdmjrsyoz1460 \\n<end_of_turn>\\n<start_of_turn>model\\n', 'target': 'Subject: Group Messaging for Admissions Process\\n\\nGood morning, everyone,\\n\\nI hope this message finds you well. As we continue our admissions processes, I would like to update you on the latest developments and key information. Please find below the timeline for our upcoming meetings:\\n\\n- [USERNAME] - Meeting at [TIME]\\n- [USERNAME] - Meeting at [TIME]\\n- [USERNAME] - Meeting at [TIME]\\n- [USERNAME] - Meeting at [TIME]\\n- [USERNAME] '}\n",
            "Valid Split ->  7946\n"
          ]
        }
      ],
      "source": [
        "# Load a PII masking dataset (expects the listed columns)\n",
        "# If you have a local file instead of HF dataset, replace with pandas read_csv and Dataset.from_pandas\n",
        "from datasets import load_dataset\n",
        "\n",
        "try:\n",
        "    pii_ds = load_dataset(\"ai4privacy/pii-masking-300k\")\n",
        "except Exception as e:\n",
        "    print(\"Falling back: please provide a local dataset with required columns.\")\n",
        "    raise e\n",
        "\n",
        "# Keep English and split by provided 'set' column if present\n",
        "if \"language\" in pii_ds[\"train\"].column_names:\n",
        "    pii_ds = pii_ds.filter(lambda ex: ex.get(\"language\", \"en\") == \"English\")\n",
        "\n",
        "# Train/validation split: if dataset has 'set' use it, else do a random split\n",
        "if \"set\" in pii_ds[\"train\"].column_names:\n",
        "    train_split = pii_ds[\"train\"].filter(lambda ex: ex.get(\"set\", \"train\") == \"train\")\n",
        "    valid_split = pii_ds[\"validation\"].filter(lambda ex: ex.get(\"set\", \"validation\") == \"validation\")\n",
        "else:\n",
        "    split = pii_ds[\"train\"].train_test_split(test_size=0.02, seed=42)\n",
        "    train_split, valid_split = split[\"train\"], split[\"test\"]\n",
        "\n",
        "needed_columns = [\n",
        "    \"source_text\", \"target_text\", \"privacy_mask\", \"span_labels\",\n",
        "    \"mbert_text_tokens\", \"mbert_bio_labels\", \"id\", \"language\", \"set\"\n",
        "]\n",
        "\n",
        "missing = [c for c in [\"source_text\", \"target_text\"] if c not in train_split.column_names]\n",
        "assert not missing, f\"Dataset missing required columns: {missing}\"\n",
        "\n",
        "# Map to prompt-target fields the model will use\n",
        "\n",
        "def add_prompt_fields(example):\n",
        "    src = example[\"source_text\"]\n",
        "    tgt = example[\"target_text\"]\n",
        "    prompt = build_pii_prompt(src)\n",
        "    example[\"prompt\"] = prompt\n",
        "    example[\"target\"] = tgt\n",
        "    return example\n",
        "\n",
        "# Apply mapping and remove original columns not in needed_columns, keeping the new 'prompt' and 'target'\n",
        "original_columns = train_split.column_names\n",
        "columns_to_remove = [col for col in original_columns if col not in needed_columns]\n",
        "\n",
        "train_split = train_split.map(add_prompt_fields, remove_columns=columns_to_remove)\n",
        "valid_split = valid_split.map(add_prompt_fields, remove_columns=columns_to_remove)\n",
        "\n",
        "\n",
        "print(\"PII dataset:\", train_split)\n",
        "print(\"Columns:\", train_split.column_names[:10])\n",
        "print({k: train_split[k][0] for k in [\"prompt\", \"target\"]})\n",
        "print(\"Valid Split -> \", len(valid_split))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JTpZy5pN_NH",
        "outputId": "09859199-6e73-44ff-a98c-2ebc4a3c6e6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train batches (PII): 29908\n"
          ]
        }
      ],
      "source": [
        "# Collate to create input_ids and labels where prompt tokens are ignored (-100)\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def tokenize_with_labels(batch, tokenizer, max_length=512):\n",
        "    prompts = batch[\"prompt\"]\n",
        "    targets = batch[\"target\"]\n",
        "\n",
        "    # Tokenize separately so we can compute boundaries\n",
        "    prompt_enc = tokenizer(prompts, padding=False, truncation=True, max_length=max_length)\n",
        "    target_enc = tokenizer(targets, padding=False, truncation=True, max_length=max_length)\n",
        "\n",
        "    input_ids = []\n",
        "    labels = []\n",
        "    attention_mask = []\n",
        "\n",
        "    for p_ids, t_ids in zip(prompt_enc[\"input_ids\"], target_enc[\"input_ids\"]):\n",
        "        # Build concatenated sequence: [prompt] + [target]\n",
        "        ids = p_ids + t_ids\n",
        "        ids = ids[:max_length]\n",
        "\n",
        "        # Build labels: -100 for prompt tokens, target token ids for target span\n",
        "        prompt_len = min(len(p_ids), max_length)\n",
        "        tgt_len = max(0, min(len(t_ids), max_length - prompt_len))\n",
        "        lab = ([-100] * prompt_len) + t_ids[:tgt_len]\n",
        "        lab = lab[:len(ids)]\n",
        "\n",
        "        am = [1] * len(ids)\n",
        "\n",
        "        input_ids.append(ids)\n",
        "        labels.append(lab)\n",
        "        attention_mask.append(am)\n",
        "\n",
        "    # Pad to max batch length\n",
        "    batch_max = max(len(x) for x in input_ids)\n",
        "    def pad_to(x, pad_id):\n",
        "        return x + [pad_id] * (batch_max - len(x))\n",
        "\n",
        "    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
        "    input_ids = [pad_to(x, pad_id) for x in input_ids]\n",
        "    labels = [pad_to(x, -100) for x in labels]\n",
        "    attention_mask = [pad_to(x, 0) for x in attention_mask]\n",
        "\n",
        "    # Create tensors on CPU first, then move to device\n",
        "    return {\n",
        "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long).to(model.device),\n",
        "        \"labels\": torch.tensor(labels, dtype=torch.long).to(model.device),\n",
        "        \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long).to(model.device),\n",
        "    }\n",
        "\n",
        "# Ensure pad token is set\n",
        "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Use the reduced BATCH_SIZE\n",
        "BATCH_SIZE = 8 # Reduced batch size\n",
        "MAX_LEN = 512\n",
        "\n",
        "train_loader_pii = DataLoader(\n",
        "    train_split, batch_size=BATCH_SIZE, shuffle=True,\n",
        "    collate_fn=lambda b: tokenize_with_labels({k: [ex[k] for ex in b] for k in b[0]}, tokenizer, max_length=MAX_LEN)\n",
        ")\n",
        "\n",
        "valid_loader_pii = DataLoader(\n",
        "    valid_split, batch_size=BATCH_SIZE, shuffle=False,\n",
        "    collate_fn=lambda b: tokenize_with_labels({k: [ex[k] for ex in b] for k in b[0]}, tokenizer, max_length=MAX_LEN)\n",
        ")\n",
        "\n",
        "print(\"Train batches (PII):\", len(train_loader_pii))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHIkDwrzN_NH"
      },
      "outputs": [],
      "source": [
        "# Training loop for PII masking (text-to-text)\n",
        "from lion_pytorch import Lion\n",
        "\n",
        "def train_pii(model, dataloader, val_loader=None, max_steps=200, learning_rate=1e-4):\n",
        "    model.train()\n",
        "    optimizer = Lion(model.parameters(), lr=learning_rate)\n",
        "    losses = []\n",
        "\n",
        "    step = 0\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            labels=batch[\"labels\"],\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        if step % 10 == 0:\n",
        "            print(f\"step {step} loss: {loss.item():.4f}\")\n",
        "\n",
        "        step += 1\n",
        "        if step >= max_steps:\n",
        "            break\n",
        "\n",
        "    return model\n",
        "\n",
        "# Kick off a short fine-tune (adjust steps as needed)\n",
        "# model = train_pii(model, train_loader_pii, valid_loader_pii, max_steps=200, learning_rate=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUCswZycN_NH"
      },
      "outputs": [],
      "source": [
        "# Inference helper for PII masking\n",
        "\n",
        "def pii_mask(text: str, max_new_tokens=128, temperature=0.0, only_answer=True):\n",
        "    prompt = build_pii_prompt(text)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        gen = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=(temperature > 0.0),\n",
        "            temperature=max(temperature, 1e-6),\n",
        "        )\n",
        "\n",
        "    out = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
        "    if only_answer:\n",
        "        # Return only the model turn after the prompt\n",
        "        if \"<start_of_turn>model\" in out:\n",
        "            out = out.split(\"<start_of_turn>model\")[-1]\n",
        "        return out.strip()\n",
        "    return out\n",
        "\n",
        "# Example:\n",
        "# print(pii_mask(\"My name is John Smith and my SSN is 123-45-6789. I live at 10 Main St, Boston MA.\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJ8nRg0zN_NH"
      },
      "outputs": [],
      "source": [
        "# PII evaluation: exact match (EM) and simple character-level F1\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_pii_exact_match(dset, n=200):\n",
        "    model.eval()\n",
        "    n = min(n, len(dset))\n",
        "    ems = []\n",
        "    for i in range(n):\n",
        "        src = dset[i][\"source_text\"]\n",
        "        tgt = dset[i][\"target_text\"]\n",
        "        pred = pii_mask(src, max_new_tokens=256, temperature=0.0, only_answer=True)\n",
        "        ems.append(1.0 if pred.strip() == tgt.strip() else 0.0)\n",
        "    em = float(np.mean(ems)) if ems else 0.0\n",
        "    print(f\"Exact Match (EM) over {n} samples: {em:.3f}\")\n",
        "    return em\n",
        "\n",
        "def char_f1(pred: str, gold: str):\n",
        "    pc = Counter(pred)\n",
        "    gc = Counter(gold)\n",
        "    overlap = sum((pc & gc).values())\n",
        "    if overlap == 0:\n",
        "        return 0.0\n",
        "    precision = overlap / max(1, sum(pc.values()))\n",
        "    recall = overlap / max(1, sum(gc.values()))\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "    return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_pii_char_f1(dset, n=200):\n",
        "    model.eval()\n",
        "    n = min(n, len(dset))\n",
        "    scores = []\n",
        "    for i in range(n):\n",
        "        src = dset[i][\"source_text\"]\n",
        "        tgt = dset[i][\"target_text\"]\n",
        "        pred = pii_mask(src, max_new_tokens=256, temperature=0.0, only_answer=True)\n",
        "        scores.append(char_f1(pred, tgt))\n",
        "    f1 = float(np.mean(scores)) if scores else 0.0\n",
        "    print(f\"Char-level F1 over {n} samples: {f1:.3f}\")\n",
        "    return f1\n",
        "\n",
        "# Example:\n",
        "# _ = evaluate_pii_exact_match(valid_split, n=100)\n",
        "# _ = evaluate_pii_char_f1(valid_split, n=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MdvFB0LSb-q",
        "outputId": "03e78d88-581d-47fa-ab32-bafcbcf5ed99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0 loss: 0.0874\n",
            "step 10 loss: 0.1280\n",
            "step 20 loss: 0.0633\n",
            "step 30 loss: 0.0749\n",
            "step 40 loss: 0.4524\n",
            "step 50 loss: 0.0464\n",
            "step 60 loss: 0.1240\n",
            "step 70 loss: 0.0729\n",
            "step 80 loss: 0.0446\n",
            "step 90 loss: 0.0762\n",
            "step 100 loss: 0.0051\n",
            "step 110 loss: 0.1852\n",
            "step 120 loss: 0.0211\n",
            "step 130 loss: 0.0585\n",
            "step 140 loss: 0.1228\n",
            "step 150 loss: 0.0466\n",
            "step 160 loss: 0.3000\n",
            "step 170 loss: 0.0100\n",
            "step 180 loss: 0.0721\n",
            "step 190 loss: 0.0003\n",
            "Fine-tuning completed.\n"
          ]
        }
      ],
      "source": [
        "FT_STEPS = 2000\n",
        "LR = 1e-4\n",
        "\n",
        "model.train()\n",
        "model = train_pii(model, train_loader_pii, valid_loader_pii, max_steps=FT_STEPS, learning_rate=LR)\n",
        "\n",
        "print(\"Fine-tuning completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LTpa6p6bt1m"
      },
      "outputs": [],
      "source": [
        "N_SHOW = 3\n",
        "for i in range(N_SHOW):\n",
        "    src = valid_split[i][\"source_text\"]\n",
        "    tgt = valid_split[i][\"target_text\"]\n",
        "    pred = pii_mask(src, max_new_tokens=256, temperature=0.0, only_answer=True)\n",
        "    print(f\"--- Sample {i} ---\")\n",
        "    print(\"Source:\\n\", src[:500])\n",
        "    print(\"\\nTarget:\\n\", tgt[:500])\n",
        "    print(\"\\nPrediction:\\n\", pred[:500])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWl0MIUidi7r",
        "outputId": "eacc2df4-3df3-4c6b-bc42-8c3975036e63"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-tuned EM: 0.000 | Base EM: 0.000\n",
            "Fine-tuned F1: 0.268 | Base F1: 0.526\n"
          ]
        }
      ],
      "source": [
        "# === Evaluation: fine-tuned vs base model (EM and char-F1) ===\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "@torch.no_grad()\n",
        "def pii_mask_with(model_obj, text: str, max_new_tokens=256):\n",
        "    prompt = build_pii_prompt(text)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_obj.device)\n",
        "    gen = model_obj.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "    )\n",
        "    out = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
        "    if \"<start_of_turn>model\" in out:\n",
        "        out = out.split(\"<start_of_turn>model\")[-1]\n",
        "    return out.strip()\n",
        "\n",
        "# Load a fresh base model (no LoRA) for comparison\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
        "\n",
        "N_EVAL = 100  # Increased from 1 to 100 for better statistical significance\n",
        "ems_ft, ems_base = [], []\n",
        "f1s_ft, f1s_base = [], []\n",
        "\n",
        "print(f\"Evaluating on {N_EVAL} samples...\\n\")\n",
        "\n",
        "for i in range(min(N_EVAL, len(valid_split))):\n",
        "    src = valid_split[i][\"source_text\"]\n",
        "    tgt = valid_split[i][\"target_text\"]\n",
        "\n",
        "    # Fine-tuned prediction (using global model)\n",
        "    pred_ft = pii_mask(src, max_new_tokens=256, temperature=0.0, only_answer=True)\n",
        "\n",
        "    # Base prediction\n",
        "    pred_base = pii_mask_with(base_model, src, max_new_tokens=256)\n",
        "\n",
        "    ems_ft.append(1.0 if pred_ft.strip() == tgt.strip() else 0.0)\n",
        "    ems_base.append(1.0 if pred_base.strip() == tgt.strip() else 0.0)\n",
        "\n",
        "    f1s_ft.append(char_f1(pred_ft, tgt))\n",
        "    f1s_base.append(char_f1(pred_base, tgt))\n",
        "    \n",
        "    # Print progress every 20 samples\n",
        "    if (i + 1) % 20 == 0:\n",
        "        print(f\"Evaluated {i + 1}/{N_EVAL} samples...\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Fine-tuned EM: {np.mean(ems_ft):.3f} | Base EM: {np.mean(ems_base):.3f}\")\n",
        "print(f\"Fine-tuned F1: {np.mean(f1s_ft):.3f} | Base F1: {np.mean(f1s_base):.3f}\")\n",
        "print(f\"\\nImprovement: {(np.mean(f1s_ft) - np.mean(f1s_base)) / np.mean(f1s_base) * 100:.1f}%\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0bb8b7d0724d4691a91b3ca251a14b10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "168042414ded436c8669b0b1366beca3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "181bb82ce4134150871ec9836bc01938": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_168042414ded436c8669b0b1366beca3",
            "placeholder": "​",
            "style": "IPY_MODEL_3047ddc9c80f4dcc8bcbdd6bee2fa083",
            "value": "Map: 100%"
          }
        },
        "23fe6f9230b24c18b6c91f7dd4bdaa2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3047ddc9c80f4dcc8bcbdd6bee2fa083": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4236993b39e941f7a59b73df72c59bcd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "423762a3ddaa448599d7120fe2294385": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a55f19eead1401196dc5acbd8646261": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5659cdd59f254996b2dd9a98bbbbb25f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c23173319385439d900101868a59de08",
            "placeholder": "​",
            "style": "IPY_MODEL_de070153197343c5814cfebc63db2cf1",
            "value": "Filter: 100%"
          }
        },
        "6053a66cd5f24d948898ef268123756d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5659cdd59f254996b2dd9a98bbbbb25f",
              "IPY_MODEL_75e3bfafa830433aace87dbf281d6e2b",
              "IPY_MODEL_95afe5626c384b8bb8ccaf3b83042dda"
            ],
            "layout": "IPY_MODEL_887eda8720584cfeb971a33a5a0bef7a"
          }
        },
        "62c188b0176b4bbf8914f69543dceda8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75e3bfafa830433aace87dbf281d6e2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a55f19eead1401196dc5acbd8646261",
            "max": 7946,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_faf3e9594b404893b07e88cb52e80d73",
            "value": 7946
          }
        },
        "86e46d5a1b904b2fac55544fcd7dfdaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f82d2ba01af041fb9cc2854032d7fa29",
              "IPY_MODEL_b9a0b6bd950b4a05b8c734c78a2ccb5b",
              "IPY_MODEL_e91d3762affb4b83ad77c8de4ed78eab"
            ],
            "layout": "IPY_MODEL_23fe6f9230b24c18b6c91f7dd4bdaa2f"
          }
        },
        "887eda8720584cfeb971a33a5a0bef7a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "894d667575eb4fcb8e365bdfe1805279": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91862fc651c3438ea2b3d3de2cdb78a2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "958837a29c124951a72ed9141b93ae62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95afe5626c384b8bb8ccaf3b83042dda": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_894d667575eb4fcb8e365bdfe1805279",
            "placeholder": "​",
            "style": "IPY_MODEL_b903ec5a163341e9b23a9c11e82869d4",
            "value": " 7946/7946 [00:02&lt;00:00, 3111.00 examples/s]"
          }
        },
        "9aa0d7c8360f470f8aa0baf79cc1191c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c13440fecd1409492fed73cefb3d65b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cba3a4eb9ff475cabf0944c043b5f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a26e5be011614cca88dbd8b408a2721f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_181bb82ce4134150871ec9836bc01938",
              "IPY_MODEL_f509e4aa420246998d6b6e110fbd8463",
              "IPY_MODEL_e63ed00e17694fc192cb5eff8126a543"
            ],
            "layout": "IPY_MODEL_62c188b0176b4bbf8914f69543dceda8"
          }
        },
        "b25aa3b07d35461bba5fff489cb02ce5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b903ec5a163341e9b23a9c11e82869d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9a0b6bd950b4a05b8c734c78a2ccb5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91862fc651c3438ea2b3d3de2cdb78a2",
            "max": 29908,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_958837a29c124951a72ed9141b93ae62",
            "value": 29908
          }
        },
        "c23173319385439d900101868a59de08": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da93275a012f4eccb3671d5f9235feb0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de070153197343c5814cfebc63db2cf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e63ed00e17694fc192cb5eff8126a543": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da93275a012f4eccb3671d5f9235feb0",
            "placeholder": "​",
            "style": "IPY_MODEL_b25aa3b07d35461bba5fff489cb02ce5",
            "value": " 7946/7946 [00:01&lt;00:00, 6416.46 examples/s]"
          }
        },
        "e91d3762affb4b83ad77c8de4ed78eab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4236993b39e941f7a59b73df72c59bcd",
            "placeholder": "​",
            "style": "IPY_MODEL_423762a3ddaa448599d7120fe2294385",
            "value": " 29908/29908 [00:05&lt;00:00, 5829.40 examples/s]"
          }
        },
        "f509e4aa420246998d6b6e110fbd8463": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bb8b7d0724d4691a91b3ca251a14b10",
            "max": 7946,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9aa0d7c8360f470f8aa0baf79cc1191c",
            "value": 7946
          }
        },
        "f82d2ba01af041fb9cc2854032d7fa29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c13440fecd1409492fed73cefb3d65b",
            "placeholder": "​",
            "style": "IPY_MODEL_9cba3a4eb9ff475cabf0944c043b5f58",
            "value": "Map: 100%"
          }
        },
        "faf3e9594b404893b07e88cb52e80d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
