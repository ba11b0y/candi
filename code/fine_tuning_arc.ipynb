{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUmiF_o0dH9B"
   },
   "source": [
    "<table align=\"center\">\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"http://introtodeeplearning.com\">\n",
    "        <img src=\"https://i.ibb.co/Jr88sn2/mit.png\" style=\"padding-bottom:5px;\" />\n",
    "      Visit MIT Deep Learning</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/MITDeepLearning/introtodeeplearning/blob/master/lab3/LLM_Finetuning.ipynb\">\n",
    "        <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
    "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/MITDeepLearning/introtodeeplearning/blob/master/lab3/LLM_Finetuning.ipynb\">\n",
    "        <img src=\"https://i.ibb.co/xfJbPmL/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
    "</table>\n",
    "\n",
    "# Copyright Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZBNhP2AodH9C"
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 MIT Introduction to Deep Learning. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the MIT License. You may not use this file except in compliance\n",
    "# with the License. Use and/or modification of this code outside of MIT Introduction\n",
    "# to Deep Learning must reference:\n",
    "#\n",
    "# © MIT Introduction to Deep Learning\n",
    "# http://introtodeeplearning.com\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rahult/ft/.venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-11-04 09:22:15.483459: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from lion_pytorch import Lion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TN2zHVhfBvnE",
    "outputId": "16e48233-d37d-4cde-c8d8-98212638bc07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_turn>user\n",
      "What is your name?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "My name is Gemma!<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Basic question-answer template\n",
    "template_without_answer = \"<start_of_turn>user\\n{question}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "template_with_answer = template_without_answer + \"{answer}<end_of_turn>\\n\"\n",
    "\n",
    "# Let's try to put something into the template to see how it looks\n",
    "print(template_with_answer.format(question=\"What is your name?\", answer=\"My name is Gemma!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EeDF1JI-BvnF",
    "outputId": "65b7375a-8a48-4aba-b16a-3e3879f5d7e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 256000\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer for Gemma 2B\n",
    "model_id = \"unsloth/gemma-2-2b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# How big is the tokenizer?\n",
    "print(f\"Vocab size: {len(tokenizer.get_vocab())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JH1XzPkiBvnF",
    "outputId": "dcf082a5-dd67-4c9b-ef37-107e2097ccb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Here is some sample text!\n",
      "Encoded tokens: tensor([[     2,   4858,    603,   1009,   6453,   2793, 235341]])\n",
      "Decoded text: Here is some sample text!\n"
     ]
    }
   ],
   "source": [
    "# Lets test out both steps:\n",
    "text = \"Here is some sample text!\"\n",
    "print(f\"Original text: {text}\")\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "print(f\"Encoded tokens: {tokens}\")\n",
    "\n",
    "# Decode the tokens\n",
    "decoded_text = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "print(f\"Decoded text: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LybwmysGdH9D"
   },
   "source": [
    "This is really cool. Now we have a way to move in and out of the token space.\n",
    "\n",
    "To \"chat\" with our LLM chatbot, we need to use the tokenizer and the chat template together, in order for the model to respond to the user's question. We can use the templates defined earlier to construct a prompt for the model, without the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jyBxl6NIBvnF",
    "outputId": "616f4858-4f4f-4ac3-954b-a196feb11575"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_turn>user\n",
      "What is the capital of France? Use one word.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = template_without_answer.format(question=\"What is the capital of France? Use one word.\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcqeF4aqdH9E"
   },
   "source": [
    "If we were to feed this to the model, it would see that it is now the start of the model's turn, and it would generate the answer to this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mWtWvgiuBvnG",
    "outputId": "cdb79dbc-5167-4559-951b-621a4da0f8fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 10,383,360 || all params: 2,624,725,248 || trainable%: 0.3956\n"
     ]
    }
   ],
   "source": [
    "# Load the model -- note that this may take a few minutes\n",
    "# Load the model -- note that this may take a few minutes\n",
    "def apply_lora(model):\n",
    "    # Define LoRA config\n",
    "    lora_config = LoraConfig(\n",
    "        r=8, # rank of the LoRA matrices\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Apply LoRA to the model\n",
    "    lora_model = get_peft_model(model, lora_config)\n",
    "    return lora_model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "# Optional: enable gradient checkpointing to save memory\n",
    "if hasattr(model, \"gradient_checkpointing_enable\"):\n",
    "    model.gradient_checkpointing_enable()\n",
    "# Resize embeddings (pad token added) and attach LoRA adapters\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = apply_lora(model)\n",
    "\n",
    "if hasattr(model, \"print_trainable_parameters\"):\n",
    "    model.print_trainable_parameters()\n",
    "else:\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable params: {trainable} / {total} ({trainable/total*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2SMDd5dpBvnG",
    "outputId": "4c27847f-9523-4609-f3e5-b8192367399d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "What does MIT stand for?\n",
      "model\n",
      "MIT stands for **Massachusetts Institute of Technology**. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Putting it together to prompt the model and generate a response ###\n",
    "\n",
    "# 1. Construct the prompt in chat template form\n",
    "question = \"What does MIT stand for?\"\n",
    "prompt = template_without_answer.format(question=question)\n",
    "\n",
    "# 2. Tokenize the prompt, including attention mask\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "# 3. Generate a sequence of tokens for the answer\n",
    "with torch.no_grad():\n",
    "    gen_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],  # Add attention mask\n",
    "        max_new_tokens=50,  # Increased tokens\n",
    "        do_sample=True,  # Enable sampling\n",
    "        temperature=0.7 # Added temperature\n",
    "    )\n",
    "\n",
    "# 4. Decode and print the full text\n",
    "print(tokenizer.decode(gen_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XnWMUQVbBvnG",
    "outputId": "18c82db0-5816-4de0-a731-d9cb0b246a64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "What does MIT stand for?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "MIT stands for **Massachusetts Institute of Technology**. \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "prompt = template_without_answer.format(question=\"What does MIT stand for?\")\n",
    "tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(tokens, max_new_tokens=20)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcg-jFy6dH9E"
   },
   "source": [
    "### 1.3.3: Forward pass and loss computation\n",
    "\n",
    "Now let's define a function to perform a forward pass through the LLM and compute the loss. The forward pass gives us the logits -- which reflect the probability distribution over the next token -- for the next token. We can compute the loss by comparing the predicted logits to the true next token -- our target label. Note that this is effectively a classification problem! So, our loss can be captured by the cross entropy loss, and we can use PyTorch's [`nn.functional.cross_entropy`](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html) function to compute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xCLtZwxwBvnI"
   },
   "outputs": [],
   "source": [
    "def forward_and_compute_loss(model, tokens, mask, context_length=512):\n",
    "    # Truncate to context length\n",
    "    tokens = tokens[:, :context_length]\n",
    "    mask = mask[:, :context_length]\n",
    "\n",
    "    # Construct the input, output, and mask\n",
    "    x = tokens[:, :-1]\n",
    "    y = tokens[:, 1:]\n",
    "    mask = mask[:, 1:]\n",
    "\n",
    "    # Forward pass to compute logits\n",
    "    logits = model(x).logits\n",
    "\n",
    "    # Compute loss\n",
    "    loss = F.cross_entropy(\n",
    "        logits.view(-1, logits.size(-1)),\n",
    "        y.view(-1),\n",
    "        reduction=\"none\"\n",
    "    )\n",
    "\n",
    "    # Mask out the loss for non-answer tokens\n",
    "    loss = loss[mask.view(-1)].mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VeoCuqmxN_NH"
   },
   "source": [
    "## PII Masking Fine-tuning (source_text ➜ target_text)\n",
    "\n",
    "Adapt the pipeline to a PII masking dataset with columns: `source_text`, `target_text`, `privacy_mask`, `span_labels`, `mbert_text_tokens`, `mbert_bio_labels`, `id`, `language`, `set`. We will fine-tune the model to transform `source_text` into its masked form `target_text`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "F28CS54wN_NH"
   },
   "outputs": [],
   "source": [
    "# Prompt template specialized for PII masking with few-shot examples\n",
    "\n",
    "def build_pii_prompt(source_text: str) -> str:\n",
    "    instruction = (\n",
    "        \"You are a data privacy assistant. Mask all personally identifiable information (PII) \"\n",
    "        \"in the following text using the masking scheme shown in the examples. \"\n",
    "        \"Output only the masked text.\\n\\n\"\n",
    "        \"Examples:\\n\\n\"\n",
    "        \"Input: My name is John Smith and my email is john.smith@email.com\\n\"\n",
    "        \"Output: My name is [FIRSTNAME] [LASTNAME] and my email is [EMAIL]\\n\\n\"\n",
    "        \"Input: Call me at 555-123-4567 or visit 123 Main Street, Boston MA 02101\\n\"\n",
    "        \"Output: Call me at [PHONENUMBER] or visit [STREET] [CITY] [STATE] [ZIPCODE]\\n\\n\"\n",
    "        \"Input: My username is alice_2023 and I was born on 03/15/1990\\n\"\n",
    "        \"Output: My username is [USERNAME] and I was born on [DOB]\\n\\n\"\n",
    "        \"Input: The SSN is 123-45-6789 and account number is ACC98765\\n\"\n",
    "        \"Output: The SSN is [SSN] and account number is [ACCOUNTNUMBER]\"\n",
    "    )\n",
    "    return (\n",
    "        f\"<start_of_turn>user\\n{instruction}\\n\\nText:\\n{source_text}\\n<end_of_turn>\\n\"\n",
    "        f\"<start_of_turn>model\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258,
     "referenced_widgets": [
      "6053a66cd5f24d948898ef268123756d",
      "5659cdd59f254996b2dd9a98bbbbb25f",
      "75e3bfafa830433aace87dbf281d6e2b",
      "95afe5626c384b8bb8ccaf3b83042dda",
      "887eda8720584cfeb971a33a5a0bef7a",
      "c23173319385439d900101868a59de08",
      "de070153197343c5814cfebc63db2cf1",
      "4a55f19eead1401196dc5acbd8646261",
      "faf3e9594b404893b07e88cb52e80d73",
      "894d667575eb4fcb8e365bdfe1805279",
      "b903ec5a163341e9b23a9c11e82869d4",
      "86e46d5a1b904b2fac55544fcd7dfdaf",
      "f82d2ba01af041fb9cc2854032d7fa29",
      "b9a0b6bd950b4a05b8c734c78a2ccb5b",
      "e91d3762affb4b83ad77c8de4ed78eab",
      "23fe6f9230b24c18b6c91f7dd4bdaa2f",
      "9c13440fecd1409492fed73cefb3d65b",
      "9cba3a4eb9ff475cabf0944c043b5f58",
      "91862fc651c3438ea2b3d3de2cdb78a2",
      "958837a29c124951a72ed9141b93ae62",
      "4236993b39e941f7a59b73df72c59bcd",
      "423762a3ddaa448599d7120fe2294385",
      "a26e5be011614cca88dbd8b408a2721f",
      "181bb82ce4134150871ec9836bc01938",
      "f509e4aa420246998d6b6e110fbd8463",
      "e63ed00e17694fc192cb5eff8126a543",
      "62c188b0176b4bbf8914f69543dceda8",
      "168042414ded436c8669b0b1366beca3",
      "3047ddc9c80f4dcc8bcbdd6bee2fa083",
      "0bb8b7d0724d4691a91b3ca251a14b10",
      "9aa0d7c8360f470f8aa0baf79cc1191c",
      "da93275a012f4eccb3671d5f9235feb0",
      "b25aa3b07d35461bba5fff489cb02ce5"
     ]
    },
    "id": "WPepdopuN_NH",
    "outputId": "a96c9410-c573-4cf4-a9cf-70936f2247ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PII dataset: Dataset({\n",
      "    features: ['source_text', 'target_text', 'privacy_mask', 'span_labels', 'mbert_text_tokens', 'mbert_bio_labels', 'id', 'language', 'set', 'prompt', 'target'],\n",
      "    num_rows: 29908\n",
      "})\n",
      "Columns: ['source_text', 'target_text', 'privacy_mask', 'span_labels', 'mbert_text_tokens', 'mbert_bio_labels', 'id', 'language', 'set', 'prompt']\n",
      "{'prompt': '<start_of_turn>user\\nYou are a data privacy assistant. Mask all personally identifiable information (PII) in the following text using the masking scheme shown in the examples. Output only the masked text.\\n\\nExamples:\\n\\nInput: My name is John Smith and my email is john.smith@email.com\\nOutput: My name is [FIRSTNAME] [LASTNAME] and my email is [EMAIL]\\n\\nInput: Call me at 555-123-4567 or visit 123 Main Street, Boston MA 02101\\nOutput: Call me at [PHONENUMBER] or visit [STREET] [CITY] [STATE] [ZIPCODE]\\n\\nInput: My username is alice_2023 and I was born on 03/15/1990\\nOutput: My username is [USERNAME] and I was born on [DOB]\\n\\nInput: The SSN is 123-45-6789 and account number is ACC98765\\nOutput: The SSN is [SSN] and account number is [ACCOUNTNUMBER]\\n\\nText:\\nSubject: Group Messaging for Admissions Process\\n\\nGood morning, everyone,\\n\\nI hope this message finds you well. As we continue our admissions processes, I would like to update you on the latest developments and key information. Please find below the timeline for our upcoming meetings:\\n\\n- wynqvrh053 - Meeting at 10:20am\\n- luka.burg - Meeting at 21\\n- qahil.wittauer - Meeting at quarter past 13\\n- gholamhossein.ruschke - Meeting at 9:47 PM\\n- pdmjrsyoz1460 \\n<end_of_turn>\\n<start_of_turn>model\\n', 'target': 'Subject: Group Messaging for Admissions Process\\n\\nGood morning, everyone,\\n\\nI hope this message finds you well. As we continue our admissions processes, I would like to update you on the latest developments and key information. Please find below the timeline for our upcoming meetings:\\n\\n- [USERNAME] - Meeting at [TIME]\\n- [USERNAME] - Meeting at [TIME]\\n- [USERNAME] - Meeting at [TIME]\\n- [USERNAME] - Meeting at [TIME]\\n- [USERNAME] '}\n",
      "Valid Split ->  7946\n"
     ]
    }
   ],
   "source": [
    "# Load a PII masking dataset (expects the listed columns)\n",
    "# If you have a local file instead of HF dataset, replace with pandas read_csv and Dataset.from_pandas\n",
    "from datasets import load_dataset\n",
    "\n",
    "try:\n",
    "    pii_ds = load_dataset(\"ai4privacy/pii-masking-300k\")\n",
    "except Exception as e:\n",
    "    print(\"Falling back: please provide a local dataset with required columns.\")\n",
    "    raise e\n",
    "\n",
    "# Keep English and split by provided 'set' column if present\n",
    "if \"language\" in pii_ds[\"train\"].column_names:\n",
    "    pii_ds = pii_ds.filter(lambda ex: ex.get(\"language\", \"en\") == \"English\")\n",
    "\n",
    "# Train/validation split: if dataset has 'set' use it, else do a random split\n",
    "if \"set\" in pii_ds[\"train\"].column_names:\n",
    "    train_split = pii_ds[\"train\"].filter(lambda ex: ex.get(\"set\", \"train\") == \"train\")\n",
    "    valid_split = pii_ds[\"validation\"].filter(lambda ex: ex.get(\"set\", \"validation\") == \"validation\")\n",
    "else:\n",
    "    split = pii_ds[\"train\"].train_test_split(test_size=0.02, seed=42)\n",
    "    train_split, valid_split = split[\"train\"], split[\"test\"]\n",
    "\n",
    "needed_columns = [\n",
    "    \"source_text\", \"target_text\", \"privacy_mask\", \"span_labels\",\n",
    "    \"mbert_text_tokens\", \"mbert_bio_labels\", \"id\", \"language\", \"set\"\n",
    "]\n",
    "\n",
    "missing = [c for c in [\"source_text\", \"target_text\"] if c not in train_split.column_names]\n",
    "assert not missing, f\"Dataset missing required columns: {missing}\"\n",
    "\n",
    "# Map to prompt-target fields the model will use\n",
    "\n",
    "def add_prompt_fields(example):\n",
    "    src = example[\"source_text\"]\n",
    "    tgt = example[\"target_text\"]\n",
    "    prompt = build_pii_prompt(src)\n",
    "    example[\"prompt\"] = prompt\n",
    "    example[\"target\"] = tgt\n",
    "    return example\n",
    "\n",
    "# Apply mapping and remove original columns not in needed_columns, keeping the new 'prompt' and 'target'\n",
    "original_columns = train_split.column_names\n",
    "columns_to_remove = [col for col in original_columns if col not in needed_columns]\n",
    "\n",
    "train_split = train_split.map(add_prompt_fields, remove_columns=columns_to_remove)\n",
    "valid_split = valid_split.map(add_prompt_fields, remove_columns=columns_to_remove)\n",
    "\n",
    "\n",
    "print(\"PII dataset:\", train_split)\n",
    "print(\"Columns:\", train_split.column_names[:10])\n",
    "print({k: train_split[k][0] for k in [\"prompt\", \"target\"]})\n",
    "print(\"Valid Split -> \", len(valid_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8JTpZy5pN_NH",
    "outputId": "09859199-6e73-44ff-a98c-2ebc4a3c6e6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches (PII): 3739\n"
     ]
    }
   ],
   "source": [
    "# Collate to create input_ids and labels where prompt tokens are ignored (-100)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def tokenize_with_labels(batch, tokenizer, max_length=512):\n",
    "    prompts = batch[\"prompt\"]\n",
    "    targets = batch[\"target\"]\n",
    "\n",
    "    # Tokenize separately so we can compute boundaries\n",
    "    prompt_enc = tokenizer(prompts, padding=False, truncation=True, max_length=max_length)\n",
    "    target_enc = tokenizer(targets, padding=False, truncation=True, max_length=max_length)\n",
    "\n",
    "    input_ids = []\n",
    "    labels = []\n",
    "    attention_mask = []\n",
    "\n",
    "    for p_ids, t_ids in zip(prompt_enc[\"input_ids\"], target_enc[\"input_ids\"]):\n",
    "        # Build concatenated sequence: [prompt] + [target]\n",
    "        ids = p_ids + t_ids\n",
    "        ids = ids[:max_length]\n",
    "\n",
    "        # Build labels: -100 for prompt tokens, target token ids for target span\n",
    "        prompt_len = min(len(p_ids), max_length)\n",
    "        tgt_len = max(0, min(len(t_ids), max_length - prompt_len))\n",
    "        lab = ([-100] * prompt_len) + t_ids[:tgt_len]\n",
    "        lab = lab[:len(ids)]\n",
    "\n",
    "        am = [1] * len(ids)\n",
    "\n",
    "        input_ids.append(ids)\n",
    "        labels.append(lab)\n",
    "        attention_mask.append(am)\n",
    "\n",
    "    # Pad to max batch length\n",
    "    batch_max = max(len(x) for x in input_ids)\n",
    "    def pad_to(x, pad_id):\n",
    "        return x + [pad_id] * (batch_max - len(x))\n",
    "\n",
    "    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n",
    "    input_ids = [pad_to(x, pad_id) for x in input_ids]\n",
    "    labels = [pad_to(x, -100) for x in labels]\n",
    "    attention_mask = [pad_to(x, 0) for x in attention_mask]\n",
    "\n",
    "    # Create tensors on CPU first, then move to device\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long).to(model.device),\n",
    "        \"labels\": torch.tensor(labels, dtype=torch.long).to(model.device),\n",
    "        \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long).to(model.device),\n",
    "    }\n",
    "\n",
    "# Ensure pad token is set\n",
    "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Use a smaller micro-batch size with gradient accumulation to save memory\n",
    "BATCH_SIZE = 2  # Micro-batch size per forward pass\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Effective batch size of 8\n",
    "MAX_LEN = 384\n",
    "\n",
    "train_loader_pii = DataLoader(\n",
    "    train_split, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    collate_fn=lambda b: tokenize_with_labels({k: [ex[k] for ex in b] for k in b[0]}, tokenizer, max_length=MAX_LEN)\n",
    ")\n",
    "\n",
    "valid_loader_pii = DataLoader(\n",
    "    valid_split, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    collate_fn=lambda b: tokenize_with_labels({k: [ex[k] for ex in b] for k in b[0]}, tokenizer, max_length=MAX_LEN)\n",
    ")\n",
    "\n",
    "print(\"Train batches (PII):\", len(train_loader_pii))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHIkDwrzN_NH"
   },
   "outputs": [],
   "source": [
    "# Training loop for PII masking (text-to-text)\n",
    "from lion_pytorch import Lion\n",
    "\n",
    "def train_pii(model, dataloader, max_steps=200, learning_rate=1e-4, gradient_accumulation_steps=1):\n",
    "    \"\"\"\n",
    "    Train the model on PII masking task.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to train\n",
    "        dataloader: Training data loader\n",
    "        max_steps: Maximum optimizer steps\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        gradient_accumulation_steps: Number of micro-batches to accumulate before an optimizer step\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer = Lion(model.parameters(), lr=learning_rate)\n",
    "    losses = []\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    micro_step = 0\n",
    "    global_step = 0\n",
    "    accum_loss = 0.0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"labels\"],\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss_value = loss.item()\n",
    "        losses.append(loss_value)\n",
    "\n",
    "        (loss / gradient_accumulation_steps).backward()\n",
    "        accum_loss += loss_value\n",
    "        micro_step += 1\n",
    "\n",
    "        if micro_step % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % 10 == 0:\n",
    "                print(f\"step {global_step} loss: {accum_loss / gradient_accumulation_steps:.4f}\")\n",
    "\n",
    "            accum_loss = 0.0\n",
    "\n",
    "            if global_step >= max_steps:\n",
    "                break\n",
    "\n",
    "    remainder = micro_step % gradient_accumulation_steps\n",
    "    if remainder != 0 and accum_loss > 0.0 and global_step < max_steps:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0:\n",
    "            print(f\"step {global_step} loss: {accum_loss / remainder:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FUCswZycN_NH"
   },
   "outputs": [],
   "source": [
    "# Inference helper for PII masking\n",
    "\n",
    "def pii_mask(text: str, max_new_tokens=128, temperature=0.0, only_answer=True):\n",
    "    prompt = build_pii_prompt(text)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=(temperature > 0.0),\n",
    "            temperature=max(temperature, 1e-6),\n",
    "        )\n",
    "\n",
    "    out = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "    if only_answer:\n",
    "        # Return only the model turn after the prompt\n",
    "        if \"<start_of_turn>model\" in out:\n",
    "            out = out.split(\"<start_of_turn>model\")[-1]\n",
    "        return out.strip()\n",
    "    return out\n",
    "\n",
    "# Example:\n",
    "# print(pii_mask(\"My name is John Smith and my SSN is 123-45-6789. I live at 10 Main St, Boston MA.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJ8nRg0zN_NH"
   },
   "outputs": [],
   "source": [
    "# PII evaluation: Character-level F1 score and detailed CSV export\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def char_f1(pred: str, gold: str):\n",
    "    \"\"\"Calculate character-level F1 score between prediction and gold text.\"\"\"\n",
    "    pc = Counter(pred)\n",
    "    gc = Counter(gold)\n",
    "    overlap = sum((pc & gc).values())\n",
    "    if overlap == 0:\n",
    "        return 0.0\n",
    "    precision = overlap / max(1, sum(pc.values()))\n",
    "    recall = overlap / max(1, sum(gc.values()))\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_and_save_f1(model_obj, dset, n=200, csv_filename='pii_evaluation_results.csv', model_name='fine-tuned'):\n",
    "    \"\"\"\n",
    "    Evaluate F1 score on dataset and save detailed results to CSV.\n",
    "    \n",
    "    Args:\n",
    "        model_obj: Model to evaluate\n",
    "        dset: Dataset to evaluate on\n",
    "        n: Number of samples to evaluate\n",
    "        csv_filename: Output CSV filename\n",
    "        model_name: Name identifier for this model (e.g., 'fine-tuned' or 'base')\n",
    "    \n",
    "    Returns:\n",
    "        avg_f1: Average F1 score\n",
    "        results_df: DataFrame with detailed results\n",
    "    \"\"\"\n",
    "    model_obj.eval()\n",
    "    n = min(n, len(dset))\n",
    "    \n",
    "    results = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    print(f\"Evaluating {model_name} model on {n} samples...\")\n",
    "    \n",
    "    for i in range(n):\n",
    "        src = dset[i][\"source_text\"]\n",
    "        tgt = dset[i][\"target_text\"]\n",
    "        \n",
    "        # Get prediction\n",
    "        if model_name == 'fine-tuned':\n",
    "            pred = pii_mask(src, max_new_tokens=256, temperature=0.0, only_answer=True)\n",
    "        else:\n",
    "            # For base model comparison\n",
    "            pred = pii_mask_with(model_obj, src, max_new_tokens=256)\n",
    "        \n",
    "        # Calculate F1\n",
    "        f1 = char_f1(pred, tgt)\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "        # Store detailed results\n",
    "        results.append({\n",
    "            'sample_id': i,\n",
    "            'model': model_name,\n",
    "            'source_text': src[:200],  # Truncate for readability\n",
    "            'target_text': tgt[:200],\n",
    "            'prediction': pred[:200],\n",
    "            'f1_score': f1,\n",
    "            'source_length': len(src),\n",
    "            'target_length': len(tgt),\n",
    "            'prediction_length': len(pred),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"  Processed {i + 1}/{n} samples...\")\n",
    "    \n",
    "    avg_f1 = float(np.mean(f1_scores)) if f1_scores else 0.0\n",
    "    \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save to CSV\n",
    "    results_df.to_csv(csv_filename, index=False)\n",
    "    print(f\"\\n✓ Results saved to: {csv_filename}\")\n",
    "    print(f\"  Average F1 Score: {avg_f1:.4f}\")\n",
    "    print(f\"  Median F1 Score: {results_df['f1_score'].median():.4f}\")\n",
    "    print(f\"  Std Dev: {results_df['f1_score'].std():.4f}\")\n",
    "    \n",
    "    return avg_f1, results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "Run the cell below to fine-tune the model on the PII masking task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_STEPS = 2000\n",
    "LR = 1e-4\n",
    "\n",
    "model.train()\n",
    "model = train_pii(\n",
    "    model,\n",
    "    train_loader_pii,\n",
    "    max_steps=FT_STEPS,\n",
    "    learning_rate=LR,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Fine-tuned Model\n",
    "\n",
    "Now that fine-tuning is complete, let's save the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save full merged model (base + LoRA combined) ===\n",
    "# This creates a ~5GB standalone model that's easier to use later\n",
    "\n",
    "print(\"Merging LoRA adapters into base model...\")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "merged_save_dir = \"./pii_merged_model\"\n",
    "print(f\"Saving model to {merged_save_dir}...\")\n",
    "\n",
    "merged_model.save_pretrained(merged_save_dir)\n",
    "tokenizer.save_pretrained(merged_save_dir)\n",
    "\n",
    "print(f\"\\n✓ Full merged model saved to: {merged_save_dir}\")\n",
    "print(f\"✓ Model size: ~5GB\")\n",
    "print(f\"\\nTo load later (simple, no LoRA needed):\")\n",
    "print(f\"  from transformers import AutoModelForCausalLM, AutoTokenizer\")\n",
    "print(f\"  model = AutoModelForCausalLM.from_pretrained('{merged_save_dir}', device_map='auto')\")\n",
    "print(f\"  tokenizer = AutoTokenizer.from_pretrained('{merged_save_dir}')\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Demo\n",
    "\n",
    "Let's test the fine-tuned model on a few validation samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "-LTpa6p6bt1m"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Caching is incompatible with gradient checkpointing in Gemma2DecoderLayer. Setting `past_key_values=None`.\n",
      "/home/rahult/ft/.venv/lib64/python3.9/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sample 0 ---\n",
      "Source:\n",
      " On the video sharing platform for educational content, a lively discussion unfolded among users from different locales within the UK.\n",
      "\n",
      "The comment thread began with paaltwvkjuijwbj957 expressing admiration for the video's insightful content, followed by 2005zheng.monckton adding a clarification on a complex topic. 43CU chimed in with a question for clarification, an\n",
      "\n",
      "Target:\n",
      " On the video sharing platform for educational content, a lively discussion unfolded among users from different locales within the UK.\n",
      "\n",
      "The comment thread began with [USERNAME] expressing admiration for the video's insightful content, followed by [USERNAME] adding a clarification on a complex topic. [USERNAME] chimed in with a question for clarification, an\n",
      "\n",
      "Prediction:\n",
      " user\n",
      "You are a data privacy assistant. Mask all personally identifiable information (PII) in the following text using the masking scheme shown in the examples. Output only the masked text.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Input: My name is John Smith and my email is john.smith@email.com\n",
      "Output: My name is [FIRSTNAME] [LASTNAME] and my email is [EMAIL]\n",
      "\n",
      "Input: Call me at 555-123-4567 or visit 123 Main Street, Boston MA 02101\n",
      "Output: Call me at [PHONENUMBER] or visit [STREET] [CITY] [STATE] [ZIPCODE]\n",
      "\n",
      "Input: My userna\n",
      "\n",
      "--- Sample 1 ---\n",
      "Source:\n",
      " d wsfdkmi9214 shared personal experiences related to the video's theme.\n",
      "\n",
      "Meanwhile, lyxmvtinlajlq99997 and ylhhhrmivzz90 engaged in a friendly debate, each presenting well-supported arguments. maria-rosaria.amardi1962 shared a thought-provoking analogy, sparking further discussion among the users.\n",
      "\n",
      "The conversation took a formal turn as 20jey.malov and D addressed e\n",
      "\n",
      "Target:\n",
      " d [USERNAME] shared personal experiences related to the video's theme.\n",
      "\n",
      "Meanwhile, [USERNAME] and [USERNAME] engaged in a friendly debate, each presenting well-supported arguments. [USERNAME] shared a thought-provoking analogy, sparking further discussion among the users.\n",
      "\n",
      "The conversation took a formal turn as [USERNAME] and [USERNAME] addressed e\n",
      "\n",
      "Prediction:\n",
      " user\n",
      "You are a data privacy assistant. Mask all personally identifiable information (PII) in the following text using the masking scheme shown in the examples. Output only the masked text.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Input: My name is John Smith and my email is john.smith@email.com\n",
      "Output: My name is [FIRSTNAME] [LASTNAME] and my email is [EMAIL]\n",
      "\n",
      "Input: Call me at 555-123-4567 or visit 123 Main Street, Boston MA 02101\n",
      "Output: Call me at [PHONENUMBER] or visit [STREET] [CITY] [STATE] [ZIPCODE]\n",
      "\n",
      "Input: My userna\n",
      "\n",
      "--- Sample 2 ---\n",
      "Source:\n",
      " ach other respectfully in their comments. yeganeh-afchar and ylhhhrmivzz90 shared additional resources related to the video's topic, enriching the discussion further.\n",
      "\n",
      "Throughout the interaction, the diverse perspectives and insights shared by individuals added depth and richness to the educational dialogue on the platform.\n",
      "\n",
      "BACKGROUND: 22:41 on December 21st, 1966\n",
      "\n",
      "Target:\n",
      " ach other respectfully in their comments. [USERNAME] and [USERNAME] shared additional resources related to the video's topic, enriching the discussion further.\n",
      "\n",
      "Throughout the interaction, the diverse perspectives and insights shared by individuals added depth and richness to the educational dialogue on the platform.\n",
      "\n",
      "BACKGROUND: [TIME] on December 21st, 1966\n",
      "\n",
      "Prediction:\n",
      " user\n",
      "You are a data privacy assistant. Mask all personally identifiable information (PII) in the following text using the masking scheme shown in the examples. Output only the masked text.\n",
      "\n",
      "Examples:\n",
      "\n",
      "Input: My name is John Smith and my email is john.smith@email.com\n",
      "Output: My name is [FIRSTNAME] [LASTNAME] and my email is [EMAIL]\n",
      "\n",
      "Input: Call me at 555-123-4567 or visit 123 Main Street, Boston MA 02101\n",
      "Output: Call me at [PHONENUMBER] or visit [STREET] [CITY] [STATE] [ZIPCODE]\n",
      "\n",
      "Input: My userna\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N_SHOW = 3\n",
    "for i in range(N_SHOW):\n",
    "    src = valid_split[i][\"source_text\"]\n",
    "    tgt = valid_split[i][\"target_text\"]\n",
    "    pred = pii_mask(src, max_new_tokens=256, temperature=0.0, only_answer=True)\n",
    "    print(f\"--- Sample {i} ---\")\n",
    "    print(\"Source:\\n\", src[:500])\n",
    "    print(\"\\nTarget:\\n\", tgt[:500])\n",
    "    print(\"\\nPrediction:\\n\", pred[:500])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "This section evaluates the fine-tuned model against the base model using **F1 scores only** (no exact match).\n",
    "\n",
    "**Generated CSV Files:**\n",
    "\n",
    "1. **`pii_finetuned_results.csv`** (from Cell 28 - Evaluation)\n",
    "   - Detailed evaluation of the fine-tuned model\n",
    "   - Columns: `sample_id`, `model`, `source_text`, `target_text`, `prediction`, `f1_score`, `source_length`, `target_length`, `prediction_length`, `timestamp`\n",
    "\n",
    "2. **`pii_base_results.csv`** (from Cell 28 - Evaluation)\n",
    "   - Same format as above, for the base model\n",
    "\n",
    "3. **`pii_comparison_results.csv`** (from Cell 28 - Evaluation)\n",
    "   - Combined results from both models for easy comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wWl0MIUidi7r",
    "outputId": "eacc2df4-3df3-4c6b-bc42-8c3975036e63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 50 samples...\n",
      "\n",
      "Evaluated 2/50 samples...\n",
      "Evaluated 4/50 samples...\n",
      "Evaluated 6/50 samples...\n",
      "Evaluated 8/50 samples...\n",
      "Evaluated 10/50 samples...\n",
      "Evaluated 12/50 samples...\n",
      "Evaluated 14/50 samples...\n",
      "Evaluated 16/50 samples...\n",
      "Evaluated 18/50 samples...\n",
      "Evaluated 20/50 samples...\n",
      "Evaluated 22/50 samples...\n",
      "Evaluated 24/50 samples...\n",
      "Evaluated 26/50 samples...\n",
      "Evaluated 28/50 samples...\n",
      "Evaluated 30/50 samples...\n",
      "Evaluated 32/50 samples...\n",
      "Evaluated 34/50 samples...\n",
      "Evaluated 36/50 samples...\n",
      "Evaluated 38/50 samples...\n",
      "Evaluated 40/50 samples...\n",
      "Evaluated 42/50 samples...\n",
      "Evaluated 44/50 samples...\n",
      "Evaluated 46/50 samples...\n",
      "Evaluated 48/50 samples...\n",
      "Evaluated 50/50 samples...\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS\n",
      "============================================================\n",
      "Fine-tuned EM: 0.000 | Base EM: 0.000\n",
      "Fine-tuned F1: 0.436 | Base F1: 0.408\n",
      "\n",
      "Improvement: 6.9%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# === Final Evaluation: Fine-tuned vs Base Model (F1 Score Comparison) ===\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "@torch.no_grad()\n",
    "def pii_mask_with(model_obj, text: str, max_new_tokens=256):\n",
    "    \"\"\"Helper function to run inference with any model.\"\"\"\n",
    "    prompt = build_pii_prompt(text)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_obj.device)\n",
    "    gen = model_obj.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    out = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "    if \"<start_of_turn>model\" in out:\n",
    "        out = out.split(\"<start_of_turn>model\")[-1]\n",
    "    return out.strip()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Evaluate fine-tuned model\n",
    "print(\"\\n1. Evaluating Fine-tuned Model...\")\n",
    "f1_ft, results_ft = evaluate_and_save_f1(\n",
    "    model, \n",
    "    valid_split, \n",
    "    n=100,  # Evaluate on 100 samples\n",
    "    csv_filename='pii_finetuned_results.csv',\n",
    "    model_name='fine-tuned'\n",
    ")\n",
    "\n",
    "# Load and evaluate base model\n",
    "print(\"\\n2. Loading and Evaluating Base Model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "f1_base, results_base = evaluate_and_save_f1(\n",
    "    base_model,\n",
    "    valid_split,\n",
    "    n=100,\n",
    "    csv_filename='pii_base_results.csv',\n",
    "    model_name='base'\n",
    ")\n",
    "\n",
    "# Combine results for side-by-side comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Fine-tuned Model F1: {f1_ft:.4f}\")\n",
    "print(f\"Base Model F1:       {f1_base:.4f}\")\n",
    "print(f\"Improvement:         {(f1_ft - f1_base):.4f} ({((f1_ft - f1_base) / f1_base * 100):.1f}%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Combine both datasets for comparison\n",
    "comparison_df = pd.concat([results_ft, results_base], ignore_index=True)\n",
    "comparison_df.to_csv('pii_comparison_results.csv', index=False)\n",
    "print(f\"\\n✓ Combined comparison saved to: pii_comparison_results.csv\")\n",
    "\n",
    "# Show some example comparisons\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAMPLE PREDICTIONS (First 3)\")   \n",
    "print(\"=\"*70)\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- Sample {i} ---\")\n",
    "    print(f\"Source: {valid_split[i]['source_text'][:150]}...\")\n",
    "    print(f\"\\nTarget: {valid_split[i]['target_text'][:150]}...\")\n",
    "    print(f\"\\nFine-tuned (F1={results_ft.iloc[i]['f1_score']:.3f}): {results_ft.iloc[i]['prediction'][:150]}...\")\n",
    "    print(f\"\\nBase Model (F1={results_base.iloc[i]['f1_score']:.3f}): {results_base.iloc[i]['prediction'][:150]}...\")\n",
    "    print(\"-\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (this-is-it)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0bb8b7d0724d4691a91b3ca251a14b10": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "168042414ded436c8669b0b1366beca3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "181bb82ce4134150871ec9836bc01938": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_168042414ded436c8669b0b1366beca3",
      "placeholder": "​",
      "style": "IPY_MODEL_3047ddc9c80f4dcc8bcbdd6bee2fa083",
      "value": "Map: 100%"
     }
    },
    "23fe6f9230b24c18b6c91f7dd4bdaa2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3047ddc9c80f4dcc8bcbdd6bee2fa083": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4236993b39e941f7a59b73df72c59bcd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "423762a3ddaa448599d7120fe2294385": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a55f19eead1401196dc5acbd8646261": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5659cdd59f254996b2dd9a98bbbbb25f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c23173319385439d900101868a59de08",
      "placeholder": "​",
      "style": "IPY_MODEL_de070153197343c5814cfebc63db2cf1",
      "value": "Filter: 100%"
     }
    },
    "6053a66cd5f24d948898ef268123756d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5659cdd59f254996b2dd9a98bbbbb25f",
       "IPY_MODEL_75e3bfafa830433aace87dbf281d6e2b",
       "IPY_MODEL_95afe5626c384b8bb8ccaf3b83042dda"
      ],
      "layout": "IPY_MODEL_887eda8720584cfeb971a33a5a0bef7a"
     }
    },
    "62c188b0176b4bbf8914f69543dceda8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75e3bfafa830433aace87dbf281d6e2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a55f19eead1401196dc5acbd8646261",
      "max": 7946,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_faf3e9594b404893b07e88cb52e80d73",
      "value": 7946
     }
    },
    "86e46d5a1b904b2fac55544fcd7dfdaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f82d2ba01af041fb9cc2854032d7fa29",
       "IPY_MODEL_b9a0b6bd950b4a05b8c734c78a2ccb5b",
       "IPY_MODEL_e91d3762affb4b83ad77c8de4ed78eab"
      ],
      "layout": "IPY_MODEL_23fe6f9230b24c18b6c91f7dd4bdaa2f"
     }
    },
    "887eda8720584cfeb971a33a5a0bef7a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "894d667575eb4fcb8e365bdfe1805279": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91862fc651c3438ea2b3d3de2cdb78a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "958837a29c124951a72ed9141b93ae62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "95afe5626c384b8bb8ccaf3b83042dda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_894d667575eb4fcb8e365bdfe1805279",
      "placeholder": "​",
      "style": "IPY_MODEL_b903ec5a163341e9b23a9c11e82869d4",
      "value": " 7946/7946 [00:02&lt;00:00, 3111.00 examples/s]"
     }
    },
    "9aa0d7c8360f470f8aa0baf79cc1191c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9c13440fecd1409492fed73cefb3d65b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9cba3a4eb9ff475cabf0944c043b5f58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a26e5be011614cca88dbd8b408a2721f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_181bb82ce4134150871ec9836bc01938",
       "IPY_MODEL_f509e4aa420246998d6b6e110fbd8463",
       "IPY_MODEL_e63ed00e17694fc192cb5eff8126a543"
      ],
      "layout": "IPY_MODEL_62c188b0176b4bbf8914f69543dceda8"
     }
    },
    "b25aa3b07d35461bba5fff489cb02ce5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b903ec5a163341e9b23a9c11e82869d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b9a0b6bd950b4a05b8c734c78a2ccb5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_91862fc651c3438ea2b3d3de2cdb78a2",
      "max": 29908,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_958837a29c124951a72ed9141b93ae62",
      "value": 29908
     }
    },
    "c23173319385439d900101868a59de08": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da93275a012f4eccb3671d5f9235feb0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de070153197343c5814cfebc63db2cf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e63ed00e17694fc192cb5eff8126a543": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da93275a012f4eccb3671d5f9235feb0",
      "placeholder": "​",
      "style": "IPY_MODEL_b25aa3b07d35461bba5fff489cb02ce5",
      "value": " 7946/7946 [00:01&lt;00:00, 6416.46 examples/s]"
     }
    },
    "e91d3762affb4b83ad77c8de4ed78eab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4236993b39e941f7a59b73df72c59bcd",
      "placeholder": "​",
      "style": "IPY_MODEL_423762a3ddaa448599d7120fe2294385",
      "value": " 29908/29908 [00:05&lt;00:00, 5829.40 examples/s]"
     }
    },
    "f509e4aa420246998d6b6e110fbd8463": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0bb8b7d0724d4691a91b3ca251a14b10",
      "max": 7946,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9aa0d7c8360f470f8aa0baf79cc1191c",
      "value": 7946
     }
    },
    "f82d2ba01af041fb9cc2854032d7fa29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c13440fecd1409492fed73cefb3d65b",
      "placeholder": "​",
      "style": "IPY_MODEL_9cba3a4eb9ff475cabf0944c043b5f58",
      "value": "Map: 100%"
     }
    },
    "faf3e9594b404893b07e88cb52e80d73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
