{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tuned vs Base Model Evaluation\n",
        "\n",
        "This notebook loads the merged fine-tuned model from disk alongside the original base model and evaluates both on the PII masking validation split.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Mitigate CUDA memory fragmentation for long-running sessions\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
        "\n",
        "torch.set_grad_enabled(False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_ID = \"unsloth/gemma-2-2b-it\"\n",
        "FINE_TUNED_DIR = Path(\"pii_merged_model\")\n",
        "DEVICE_MAP = \"auto\"\n",
        "TORCH_DTYPE = torch.float16\n",
        "MAX_NEW_TOKENS = 256\n",
        "\n",
        "if not FINE_TUNED_DIR.exists():\n",
        "    raise FileNotFoundError(f\"Expected fine-tuned model at {FINE_TUNED_DIR.resolve()}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(FINE_TUNED_DIR)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_pii_prompt(source_text: str) -> str:\n",
        "    instruction = (\n",
        "        \"You are a data privacy assistant. Mask all personally identifiable information (PII) \"\n",
        "        \"in the following text using the masking scheme shown in the examples. \"\n",
        "        \"Output only the masked text.\\n\\n\"\n",
        "        \"Examples:\\n\\n\"\n",
        "        \"Input: My name is John Smith and my email is john.smith@email.com\\n\"\n",
        "        \"Output: My name is [FIRSTNAME] [LASTNAME] and my email is [EMAIL]\\n\\n\"\n",
        "        \"Input: Call me at 555-123-4567 or visit 123 Main Street, Boston MA 02101\\n\"\n",
        "        \"Output: Call me at [PHONENUMBER] or visit [STREET] [CITY] [STATE] [ZIPCODE]\\n\\n\"\n",
        "        \"Input: My username is alice_2023 and I was born on 03/15/1990\\n\"\n",
        "        \"Output: My username is [USERNAME] and I was born on [DOB]\\n\\n\"\n",
        "        \"Input: The SSN is 123-45-6789 and account number is ACC98765\\n\"\n",
        "        \"Output: The SSN is [SSN] and account number is [ACCOUNTNUMBER]\"\n",
        "    )\n",
        "    return (\n",
        "        f\"<start_of_turn>user\\n{instruction}\\n\\nText:\\n{source_text}\\n<end_of_turn>\\n\"\n",
        "        f\"<start_of_turn>model\\n\"\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_validation_split() -> \"datasets.arrow_dataset.Dataset\":\n",
        "    pii_ds = load_dataset(\"ai4privacy/pii-masking-300k\")\n",
        "\n",
        "    if \"language\" in pii_ds[\"train\"].column_names:\n",
        "        pii_ds = pii_ds.filter(lambda ex: ex.get(\"language\", \"English\") == \"English\")\n",
        "\n",
        "    if \"set\" in pii_ds[\"train\"].column_names and \"validation\" in pii_ds:\n",
        "        valid_split = pii_ds[\"validation\"].filter(\n",
        "            lambda ex: ex.get(\"set\", \"validation\") == \"validation\"\n",
        "        )\n",
        "    else:\n",
        "        split = pii_ds[\"train\"].train_test_split(test_size=0.02, seed=42)\n",
        "        valid_split = split[\"test\"]\n",
        "\n",
        "    required = {\"source_text\", \"target_text\"}\n",
        "    missing = required.difference(valid_split.column_names)\n",
        "    if missing:\n",
        "        raise KeyError(f\"Validation split missing columns: {sorted(missing)}\")\n",
        "\n",
        "    def add_prompt_fields(example):\n",
        "        example = dict(example)\n",
        "        example[\"prompt\"] = build_pii_prompt(example[\"source_text\"])\n",
        "        example[\"target\"] = example[\"target_text\"]\n",
        "        return example\n",
        "\n",
        "    valid_split = valid_split.map(add_prompt_fields)\n",
        "    return valid_split\n",
        "\n",
        "\n",
        "valid_split = load_validation_split()\n",
        "print(f\"Validation examples: {len(valid_split)}\")\n",
        "print({key: valid_split[key][0][:200] for key in [\"prompt\", \"target\"]})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def char_f1(pred: str, gold: str) -> float:\n",
        "    pc = Counter(pred)\n",
        "    gc = Counter(gold)\n",
        "    overlap = sum((pc & gc).values())\n",
        "    if overlap == 0:\n",
        "        return 0.0\n",
        "    precision = overlap / max(1, sum(pc.values()))\n",
        "    recall = overlap / max(1, sum(gc.values()))\n",
        "    if precision + recall == 0:\n",
        "        return 0.0\n",
        "    return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "\n",
        "def compute_masking_metrics(prediction: str, spans) -> dict:\n",
        "    total_spans = len(spans) if spans is not None else 0\n",
        "    leaked_values = []\n",
        "    gold_label_counts = Counter()\n",
        "\n",
        "    if spans is None:\n",
        "        spans = []\n",
        "\n",
        "    normalized_spans = []\n",
        "    for span in spans:\n",
        "        if isinstance(span, dict):\n",
        "            normalized_spans.append(span)\n",
        "        elif isinstance(span, (list, tuple)):\n",
        "            value = span[0] if len(span) > 0 else \"\"\n",
        "            label = span[1] if len(span) > 1 else \"\"\n",
        "            normalized_spans.append({\"value\": value, \"label\": label})\n",
        "        elif isinstance(span, str):\n",
        "            normalized_spans.append({\"value\": span, \"label\": \"\"})\n",
        "\n",
        "    lower_prediction = prediction.lower()\n",
        "    for span in normalized_spans:\n",
        "        label = span.get(\"label\", \"\") or \"\"\n",
        "        value = span.get(\"value\", \"\") or \"\"\n",
        "        gold_label_counts[label] += 1\n",
        "        if value and value.lower() in lower_prediction:\n",
        "            leaked_values.append(value)\n",
        "\n",
        "    total_spans = len(normalized_spans)\n",
        "\n",
        "    pred_labels = re.findall(r\"\\[([A-Z0-9_]+)\\]\", prediction)\n",
        "    pred_label_counts = Counter(pred_labels)\n",
        "\n",
        "    matched_placeholders = sum(\n",
        "        min(pred_label_counts[label], gold_label_counts[label])\n",
        "        for label in gold_label_counts\n",
        "    )\n",
        "\n",
        "    placeholder_precision = (\n",
        "        matched_placeholders / max(1, sum(pred_label_counts.values()))\n",
        "        if pred_label_counts\n",
        "        else 1.0 if total_spans == 0 else 0.0\n",
        "    )\n",
        "    placeholder_recall = matched_placeholders / max(1, total_spans)\n",
        "\n",
        "    masked_spans = total_spans - len(leaked_values)\n",
        "    mask_recall = masked_spans / total_spans if total_spans > 0 else 1.0\n",
        "\n",
        "    extra_placeholders = sum(\n",
        "        max(pred_label_counts[label] - gold_label_counts.get(label, 0), 0)\n",
        "        for label in pred_label_counts\n",
        "    )\n",
        "    missing_placeholders = total_spans - matched_placeholders\n",
        "\n",
        "    return {\n",
        "        \"total_spans\": total_spans,\n",
        "        \"masked_spans\": masked_spans,\n",
        "        \"mask_recall\": mask_recall,\n",
        "        \"placeholder_precision\": placeholder_precision,\n",
        "        \"placeholder_recall\": placeholder_recall,\n",
        "        \"extra_placeholders\": extra_placeholders,\n",
        "        \"missing_placeholders\": max(missing_placeholders, 0),\n",
        "        \"leaked_values\": leaked_values,\n",
        "        \"gold_label_counts\": gold_label_counts,\n",
        "        \"pred_label_counts\": pred_label_counts,\n",
        "    }\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_masked_text(model: AutoModelForCausalLM, source_text: str) -> str:\n",
        "    prompt = build_pii_prompt(source_text)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    gen = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=MAX_NEW_TOKENS,\n",
        "        do_sample=False,\n",
        "    )\n",
        "    out = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
        "    if \"<start_of_turn>model\" in out:\n",
        "        out = out.split(\"<start_of_turn>model\")[-1]\n",
        "    return out.strip()\n",
        "\n",
        "\n",
        "def evaluate_model(\n",
        "    model: AutoModelForCausalLM,\n",
        "    dataset,\n",
        "    model_name: str,\n",
        "    n_samples: int = 200,\n",
        "    csv_path: Optional[Path] = None,\n",
        "):\n",
        "    model.eval()\n",
        "    n = min(n_samples, len(dataset))\n",
        "    rows = []\n",
        "    f1_scores = []\n",
        "    mask_recalls = []\n",
        "    placeholder_precisions = []\n",
        "    placeholder_recalls = []\n",
        "\n",
        "    print(f\"Evaluating {model_name} on {n} samples...\")\n",
        "    for idx in range(n):\n",
        "        sample = dataset[idx]\n",
        "        source = sample[\"source_text\"]\n",
        "        target = sample[\"target_text\"]\n",
        "        spans = sample.get(\"span_labels\")\n",
        "\n",
        "        prediction = generate_masked_text(model, source)\n",
        "        f1 = char_f1(prediction, target)\n",
        "        metrics = compute_masking_metrics(prediction, spans)\n",
        "\n",
        "        f1_scores.append(f1)\n",
        "        mask_recalls.append(metrics[\"mask_recall\"])\n",
        "        placeholder_precisions.append(metrics[\"placeholder_precision\"])\n",
        "        placeholder_recalls.append(metrics[\"placeholder_recall\"])\n",
        "\n",
        "        rows.append(\n",
        "            {\n",
        "                \"sample_id\": idx,\n",
        "                \"model\": model_name,\n",
        "                \"source_text\": source,\n",
        "                \"target_text\": target,\n",
        "                \"prediction\": prediction,\n",
        "                \"f1_score\": f1,\n",
        "                \"mask_recall\": metrics[\"mask_recall\"],\n",
        "                \"placeholder_precision\": metrics[\"placeholder_precision\"],\n",
        "                \"placeholder_recall\": metrics[\"placeholder_recall\"],\n",
        "                \"total_spans\": metrics[\"total_spans\"],\n",
        "                \"masked_spans\": metrics[\"masked_spans\"],\n",
        "                \"extra_placeholders\": metrics[\"extra_placeholders\"],\n",
        "                \"missing_placeholders\": metrics[\"missing_placeholders\"],\n",
        "                \"leaked_count\": len(metrics[\"leaked_values\"]),\n",
        "                \"leaked_values\": \" || \".join(metrics[\"leaked_values\"]),\n",
        "                \"source_length\": len(source),\n",
        "                \"target_length\": len(target),\n",
        "                \"prediction_length\": len(prediction),\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "            }\n",
        "        )\n",
        "        if (idx + 1) % 20 == 0:\n",
        "            print(f\"  Processed {idx + 1}/{n}\")\n",
        "\n",
        "    avg_f1 = float(sum(f1_scores) / n) if n > 0 else 0.0\n",
        "    avg_mask_recall = float(sum(mask_recalls) / n) if n > 0 else 0.0\n",
        "    avg_placeholder_precision = float(sum(placeholder_precisions) / n) if n > 0 else 0.0\n",
        "    avg_placeholder_recall = float(sum(placeholder_recalls) / n) if n > 0 else 0.0\n",
        "\n",
        "    results_df = pd.DataFrame(rows)\n",
        "    if csv_path is not None:\n",
        "        results_df.to_csv(csv_path, index=False)\n",
        "        print(f\"Saved detailed results to {csv_path}\")\n",
        "\n",
        "    print(\n",
        "        f\"Average F1 ({model_name}): {avg_f1:.4f}\\n\"\n",
        "        f\"Mask recall ({model_name}): {avg_mask_recall:.4f}\\n\"\n",
        "        f\"Placeholder precision ({model_name}): {avg_placeholder_precision:.4f}\\n\"\n",
        "        f\"Placeholder recall ({model_name}): {avg_placeholder_recall:.4f}\\n\"\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        avg_f1,\n",
        "        avg_mask_recall,\n",
        "        avg_placeholder_precision,\n",
        "        avg_placeholder_recall,\n",
        "        results_df,\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=DEVICE_MAP,\n",
        "    torch_dtype=TORCH_DTYPE,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
        "    FINE_TUNED_DIR,\n",
        "    device_map=DEVICE_MAP,\n",
        "    torch_dtype=TORCH_DTYPE,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "for mdl, name in [(base_model, \"Base\"), (fine_tuned_model, \"Fine-tuned\")]:\n",
        "    mdl.eval()\n",
        "    print(f\"Loaded {name} model on: {mdl.device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EVAL_SAMPLES = 100\n",
        "OUTPUT_DIR = Path(\".\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "fine_tuned_csv = OUTPUT_DIR / \"pii_finetuned_results_eval.csv\"\n",
        "base_csv = OUTPUT_DIR / \"pii_base_results_eval.csv\"\n",
        "comparison_csv = OUTPUT_DIR / \"pii_comparison_results_eval.csv\"\n",
        "\n",
        "(\n",
        "    fine_avg_f1,\n",
        "    fine_mask_recall,\n",
        "    fine_placeholder_precision,\n",
        "    fine_placeholder_recall,\n",
        "    fine_df,\n",
        ") = evaluate_model(\n",
        "    fine_tuned_model,\n",
        "    valid_split,\n",
        "    model_name=\"fine-tuned\",\n",
        "    n_samples=EVAL_SAMPLES,\n",
        "    csv_path=fine_tuned_csv,\n",
        ")\n",
        "\n",
        "(\n",
        "    base_avg_f1,\n",
        "    base_mask_recall,\n",
        "    base_placeholder_precision,\n",
        "    base_placeholder_recall,\n",
        "    base_df,\n",
        ") = evaluate_model(\n",
        "    base_model,\n",
        "    valid_split,\n",
        "    model_name=\"base\",\n",
        "    n_samples=EVAL_SAMPLES,\n",
        "    csv_path=base_csv,\n",
        ")\n",
        "\n",
        "delta_f1 = fine_avg_f1 - base_avg_f1\n",
        "print(f\"Fine-tuned improvement over base (F1): {delta_f1:.4f}\")\n",
        "\n",
        "comparison_df = pd.DataFrame(\n",
        "    {\n",
        "        \"model\": [\"fine-tuned\", \"base\"],\n",
        "        \"avg_f1\": [fine_avg_f1, base_avg_f1],\n",
        "        \"mask_recall\": [fine_mask_recall, base_mask_recall],\n",
        "        \"placeholder_precision\": [\n",
        "            fine_placeholder_precision,\n",
        "            base_placeholder_precision,\n",
        "        ],\n",
        "        \"placeholder_recall\": [\n",
        "            fine_placeholder_recall,\n",
        "            base_placeholder_recall,\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "comparison_df.to_csv(comparison_csv, index=False)\n",
        "print(f\"Saved comparison summary to {comparison_csv}\")\n",
        "comparison_df\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
