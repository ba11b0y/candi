%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Template for USENIX papers.
%
% History:
%
% - TEMPLATE for Usenix papers, specifically to meet requirements of
%   USENIX '05. originally a template for producing IEEE-format
%   articles using LaTeX. written by Matthew Ward, CS Department,
%   Worcester Polytechnic Institute. adapted by David Beazley for his
%   excellent SWIG paper in Proceedings, Tcl 96. turned into a
%   smartass generic template by De Clarke, with thanks to both the
%   above pioneers. Use at your own risk. Complaints to /dev/null.
%   Make it two column with no page numbering, default is 10 point.
%
% - Munged by Fred Douglis <douglis@research.att.com> 10/97 to
%   separate the .sty file from the LaTeX source template, so that
%   people can more easily include the .sty file into an existing
%   document. Also changed to more closely follow the style guidelines
%   as represented by the Word sample file.
%
% - Note that since 2010, USENIX does not require endnotes. If you
%   want foot of page notes, don't include the endnotes package in the
%   usepackage command, below.
% - This version uses the latex2e styles, not the very ancient 2.09
%   stuff.
%
% - Updated July 2018: Text block size changed from 6.5" to 7"
%
% - Updated Dec 2018 for ATC'19:
%
%   * Revised text to pass HotCRP's auto-formatting check, with
%     hotcrp.settings.submission_form.body_font_size=10pt, and
%     hotcrp.settings.submission_form.line_height=12pt
%
%   * Switched from \endnote-s to \footnote-s to match Usenix's policy.
%
%   * \section* => \begin{abstract} ... \end{abstract}
%
%   * Make template self-contained in terms of bibtex entires, to allow
%     this file to be compiled. (And changing refs style to 'plain'.)
%
%   * Make template self-contained in terms of figures, to
%     allow this file to be compiled. 
%
%   * Added packages for hyperref, embedding fonts, and improving
%     appearance.
%   
%   * Removed outdated text.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix}

% to be able to draw some self-contained figs
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{array}

% Use external bibliography file

%-------------------------------------------------------------------------------
\begin{document}
%-------------------------------------------------------------------------------

%don't want date printed
\date{}

% make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{\Large \bf Understanding Privacy Preserving Knowledge in models via Mechanistic Interpretability}

%for single author (just remove % characters)
\author{
{\rm Harith Laxman}\\
Virginia Tech
\and
{\rm Rahul Tiwari}\\
Virginia Tech
% copy the following lines to add more authors
% \and
% {\rm Name}\\
%Name Institution
} % end author

\maketitle

%-------------------------------------------------------------------------------
\begin{abstract}
%-------------------------------------------------------------------------------
Large Language Models (LLMs) have become increasingly capable of performing complex tasks over time. This has led to their widespread integration across diverse fields such as coding, education, legal assistance, and even customer support in the form of AI agents. With their increased adoption, they now have access to vast amounts of data, including sensitive information. Previous research \cite{cywinski2025towards} has shown that certain white-box approaches like mechanistic interpretability can be used to elicit latent knowledge from them. Most work, however, uses model organisms\cite{marks2025auditing} where these LLMs are specifically trained to hold secrets, while in real-world scenarios, these LLMs are not explicitly trained to hold secrets but are instead trained with alignment and privacy-preserving techniques post-training.
\end{abstract}


%-------------------------------------------------------------------------------
\section{Problem Definition}
%-------------------------------------------------------------------------------

TODO: Add a more solid problem definition.
%-------------------------------------------------------------------------------
\section{Proposed Method}
%-------------------------------------------------------------------------------

This project aims to study the privacy-preserving properties of LLMs with a real-world application in mind. The idea is to simulate a post-training scenario where a chosen language model is trained on a dataset of text to redact PII (Personally Identifiable Information) and then use techniques from \textit{Mechanistic Interpretability} such as activation analysis (where top activation neurons are examined) and circuit analysis (where a combination of neurons in a vector subspace are analyzed) to see if the model still elicits the PII even after being trained to redact it.

\subsection*{Dataset}
The project aims to build on a subset of the \textbf{AI4Privacy PII Masking Dataset}\cite{ai4privacy_2024}. The dataset contains a large number of text records with PII masked and a sample record from the dataset is shown in Table \ref{table:sample_record}. The dataset is structured as follows:
\begin{itemize}
  \item \texttt{source\_text}: raw text with embedded PII,
  \item \texttt{target\_text}: masked text with placeholders (e.g., [USERNAME], [TIME]),
  \item \texttt{privacy\_mask} and \texttt{span\_labels}: span-level annotations,
  \item \texttt{mberttokens} and \texttt{BIO labels} for sequence tagging.
\end{itemize}

\begin{table}[h!]
  \centering
  \begin{tabular}{|l|p{0.7\columnwidth}|}
    \hline
    \textbf{Column} & \textbf{Sample Entry} \\
    \hline
    source\_text & Subject: Group Messaging for Admissions Process - Good morning, everyone. Users: \textcolor{red}{wynqvrh053}, \textcolor{red}{luka.burg}, \textcolor{red}{qahil.wittauer}, \textcolor{red}{gholamhossein.ruschke}, \textcolor{red}{pdmjrsyoz1460}. Times: \textcolor{blue}{10:20am}, \textcolor{blue}{21}, \textcolor{blue}{quarter past 13}, \textcolor{blue}{9:47 PM} \\
    \hline
    target\_text & Subject: Group Messaging for Admissions Process - Good morning, everyone. Users: [\textcolor{red}{USERNAME}], [\textcolor{red}{USERNAME}], [\textcolor{red}{USERNAME}], [\textcolor{red}{USERNAME}], [\textcolor{red}{USERNAME}]. Times: [\textcolor{blue}{TIME}], [\textcolor{blue}{TIME}], [\textcolor{blue}{TIME}], [\textcolor{blue}{TIME}] \\
    \hline
    privacy\_mask & [{"value": "wynqvrh053", "start": 287, "end": 297, "label": "USERNAME"}, {"value": "10:20am", "start": 311, "end": 318, "label": "TIME"}, ...] \\
    \hline
    span\_labels & [[440, 453, "USERNAME"], [430, 437, "TIME"], [395, 416, "USERNAME"], ...] \\
    \hline
    mbert\_text\_tokens & ["Sub", "\#ject", ":", "Group", "Mess", "\#aging", ..., "\#60"] \\
    \hline
    mbert\_bio\_labels & ["O", "O", ..., "B-USERNAME", "I-USERNAME"] \\
    \hline
    id & 40767A \\
    \hline
    language & English \\
    \hline
    set & train \\
    \hline
  \end{tabular}
  \caption{An example record from the PII dataset with sensitive fields highlighted.}
  \label{table:sample_record}
\end{table}

\subsection*{Training}
We plan to train a small open-source model (e.g., \texttt{Gemma-2b-it}\cite{gemma_2b_it}) using parameter-efficient fine-tuning such as LoRA\cite{hu2022lora} or PEFT\cite{peft_huggingface} on PII-masking pairs to learn structured redaction. To ensure the model training is computationally feasible, we tested fine-tuning the model on a dummy dataset as part of the Intro to Deep Learning course by MIT (the collab notebook is available at \cite{llm_finetuning_mit}).

%-------------------------------------------------------------------------------
\section{Timeline}
%-------------------------------------------------------------------------------

\begin{center}
  \begin{tabular}{p{0.25\columnwidth}p{0.7\columnwidth}}
    \textbf{Week 1–2:} & Literature review on PII masking and fine-tuning LLMs.\\
    \textbf{Week 3–4:} & Dataset preprocessing and LoRA baseline fine-tuning on the subset of the dataset.\\
    \textbf{Week 5–6:} & Begin activation analysis using logit lens and then circuit analysis.\\
    \textbf{Week 7–8:} & Conduct experiments and analyze the results.\\
    \textbf{Week 9:} & Write final report and prepare reproducible code repository.\\
  \end{tabular}
\end{center}


\bibliographystyle{plain}
\bibliography{ref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%  LocalWords:  endnotes includegraphics fread ptr nobj noindent
%%  LocalWords:  pdflatex acks
